{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b6dec-c0f6-4496-9b75-d74bcba07183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla \n",
    "import math \n",
    "import random \n",
    "import time \n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "import random \n",
    "from yaml.loader import SafeLoader\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f663bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the second line to the path of your weights \n",
    "# this is what starts the onnx runtime \n",
    "session = ort.InferenceSession(\n",
    "        '/home/ageda/projects/Yolov8/runs/detect/ShehrozThesisYolov8/weights/ShehrozThesisYolov8.onnx',\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "input_type = session.get_inputs()[0].type\n",
    "output_name = session.get_outputs()[0].name\n",
    "output_shape = session.get_outputs()[0].shape\n",
    "output_type = session.get_outputs()[0].type\n",
    "# this is just to see what everything looks like, can be removed \n",
    "\n",
    "print(\"input name\", input_name)\n",
    "print(\"input shape\", input_shape)\n",
    "print(\"input type\", input_type)\n",
    "print(\"output name\", output_name)\n",
    "print(\"output shape\", output_shape)\n",
    "print(\"output type\", output_type)\n",
    "# same as the previous code but a different shorter way to achieve it\n",
    "\n",
    "outname = [i.name for i in session.get_outputs()]\n",
    "inname = [i.name for i in session.get_inputs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e786f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect the client and set up bp library and spawn points\n",
    "# standard carla set up \n",
    "client = carla.Client('localhost', 2000) \n",
    "world = client.get_world()\n",
    "bp_lib = world.get_blueprint_library() \n",
    "spawn_points = world.get_map().get_spawn_points() \n",
    "settings = world.get_settings()\n",
    "settings.fixed_delta_seconds = 0.01\n",
    "world.apply_settings(settings)\n",
    "# path to your yaml file\n",
    "\n",
    "with open('data_custom.yaml', mode='r') as f:\n",
    "    data_yaml = yaml.load(f,Loader=SafeLoader)\n",
    "CLASSES = data_yaml['names']\n",
    "\n",
    "print('The Classes are as follows:',CLASSES)\n",
    "\n",
    "# thresholds, change according to your use case \n",
    "# or dont! \n",
    "conf_threshold = 0.5\n",
    "class_threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback stores sensor data in a dictionary for use outside callback    \n",
    "# also used for object detection  \n",
    "def camera_callback(image, data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    \n",
    "    \n",
    "    # need the source image to tell opencv what to display it on\n",
    "    # originalImage is the source image coming from the camera sensor\n",
    "    originalImage = data_dict['image']\n",
    "    # ONNX expects things in a particular format\n",
    "    # so the CARLA camera data needs to be reshaped accordingly\n",
    "    # first remove the alpha channel\n",
    "    \n",
    "    frame = cv2.cvtColor(data_dict['image'], cv2.COLOR_RGBA2RGB)\n",
    "    # then transpose it \n",
    "\n",
    "    transposed_image = frame.transpose((2,0,1))\n",
    "    # expand the dimensions, sounds cool but its not \n",
    "    expanded_dimensions = np.expand_dims(transposed_image,0)\n",
    "    # change to float because it does not like int \n",
    "\n",
    "    inputImage = expanded_dimensions.astype(np.float32)\n",
    "    # normalize because everyone likes nice numbers\n",
    "\n",
    "    inputImage /= 255\n",
    "    # start infering with ONNX\n",
    "\n",
    "    output = session.run(outname, {input_name: inputImage})[0]\n",
    "\n",
    "    #[8.96539593e+00 5.91810465e-01 1.15975008e+01 9.05570602e+00\n",
    "    #0.00000000e+00 0.00000000e+00 0.00000000e+00 4.76837158e-07\n",
    "    #0.00000000e+00 0.00000000e+00 1.06977284e-01 4.38958406e-03\n",
    "     #9.98617768e-01 2.87176967e-02] these are the coordinates of the bounding boxes\n",
    "     \n",
    "    outputs = np.transpose(np.squeeze(output))\n",
    "    rows = outputs.shape[0]\n",
    "    boxes = []\n",
    "    scores = []\n",
    "    class_ids = []\n",
    "\n",
    "    for i in range(rows):\n",
    "        classes_scores = outputs[i][4:]\n",
    "        # get the best score \n",
    "        max_score = np.amax(classes_scores)\n",
    "        if max_score >= conf_threshold:\n",
    "                # Get the class index of the highest score\n",
    "                class_id = np.argmax(classes_scores)\n",
    "\n",
    "                # Extract the bounding box coordinates from the current row\n",
    "                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]\n",
    "\n",
    "                # Calculate the scaled coordinates of the bounding box\n",
    "                left = int((x - w / 2) )\n",
    "                top = int((y - h / 2) )\n",
    "                width = int(w)\n",
    "                height = int(h)\n",
    "\n",
    "                # Add the class ID, score, and box coordinates to the respective lists\n",
    "                class_ids.append(class_id)\n",
    "                scores.append(max_score)\n",
    "                boxes.append([left, top, width, height])\n",
    "    #apply NMS\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, 0.5, 0.35)\n",
    "    # extract the predictions post NMS\n",
    "    # now you are ready to display everything\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        score = scores[i]\n",
    "        class_id = class_ids[i]\n",
    "        name = CLASSES[class_id]\n",
    "        x1, y1, w, h = box\n",
    "        DisplayedScore = int(score*100)\n",
    "        text = f'{name}: {DisplayedScore}%'\n",
    "        # you can comment this out if you dont want the text displayed\n",
    "        print(text)\n",
    "        #print('coordinates',x1,y1,'for',text)\n",
    "        cv2.rectangle(originalImage, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)),(0,0,0) , 2)\n",
    "        \n",
    "        # -5 seemed like a good height to display the text\n",
    "        # adjust it if you dont like it\n",
    "        cv2.putText(originalImage,text,(int(x1),int(y1-5)),cv2.FONT_HERSHEY_PLAIN,0.7,(255,255,255),1)\n",
    "        # slows down the simulation in order to show the predictions, otherwise it goes fast fast \n",
    "\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c94c6a-4c3e-487b-8de9-823e440f3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn ego vehicle, rgb camera and pedestrians \n",
    "vehicle_bp = bp_lib.find('vehicle.audi.a2') \n",
    "vehicle = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "# Move spectator behind vehicle to view\n",
    "spectator = world.get_spectator() \n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=1,z=2)),vehicle.get_transform().rotation) \n",
    "spectator.set_transform(transform)\n",
    "camera_bp = bp_lib.find('sensor.camera.rgb') \n",
    "camera_bp.set_attribute('image_size_x', '640')\n",
    "camera_bp.set_attribute('image_size_y', '640')\n",
    "# this location is important, if you set it too close\n",
    "# the object detection starts predicting the ego vehicle as\n",
    "# a car, which is not what i want\n",
    "camera_init_trans = carla.Transform(carla.Location(x=1,z=2)) #Change this to move camera\n",
    "camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "# time.sleep cuz carla is a bit slow slow \n",
    "# allows carla to catch up with the camera being initialized\n",
    "time.sleep(0.2)\n",
    "spectator.set_transform(camera.get_transform())\n",
    "# Get camera dimensions and initialise dictionary                       \n",
    "image_w = camera_bp.get_attribute(\"image_size_x\").as_int()\n",
    "image_h = camera_bp.get_attribute(\"image_size_y\").as_int()\n",
    "\n",
    "camera_data = {'image': np.zeros((image_h, image_w, 4))}\n",
    "print('Camera width is ',image_w,' and camera height is ',image_h)\n",
    "time.sleep(0.2)\n",
    "# spawn 50 vehicles \n",
    "\n",
    "for i in range(50): \n",
    "    vehicle_bp = random.choice(bp_lib.filter('vehicle')) \n",
    "    npc = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points)) \n",
    "for v in world.get_actors().filter('*vehicle*'): \n",
    "    v.set_autopilot(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what you use to start the object detection \n",
    "\n",
    "camera.listen(lambda image: camera_callback(image, camera_data))\n",
    "cv2.waitKey(1)\n",
    "\n",
    "# Game loop\n",
    "while True:\n",
    "    \n",
    "\n",
    "    # Imshow renders sensor data to display\n",
    "    cv2.imshow('YOLOv8 Detections', camera_data['image'])\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# Close OpenCV window when finished\n",
    "cv2.destroyAllWindows()\n",
    "# tadaaa, everything works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this after you have closed the opencv predictions window\n",
    "# CARLA has some crazy memory leaks \n",
    "camera.destroy()\n",
    "for v in world.get_actors().filter('*vehicle*'): \n",
    "    v.destroy()\n",
    "\n",
    "\n",
    "#### Below is an excerpt from the documentation explaining what\n",
    "# the ONNX model actually outputs\n",
    "# fun fact i guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15847c0b",
   "metadata": {},
   "source": [
    "outputs: This variable contains the output data produced by the ONNX model during inference. The run method returns a tuple of outputs for the model, with each output represented as a NumPy array. In this case, we are only interested in the first output, which is why we use [0] to access it.\n",
    "To run inference on the model, we pass the input tensor data as a dictionary to the run method, where the keys are the names of the input nodes for the model (in this case, input_names[0]), and the values are the input tensor data. We also provide a list of output node names (output_names) so that the run method knows which outputs to return. Once the run method is called, it performs inference on the model and returns the output data in the form of NumPy arrays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
